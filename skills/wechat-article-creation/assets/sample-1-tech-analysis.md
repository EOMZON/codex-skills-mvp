# 样本1：技术分析类（开头节奏参考）

DeepSeek又发论文了。

这次的主题有点意思：他们发现，现在的大模型在浪费大量算力做一件很傻的事——用计算来模拟查字典。

论文叫《Conditional Memory via Scalable Lookup》，核心是一个叫 Engram 的模块。

这个名字有点意思。Engram 是神经科学术语，最早由德国生物学家 Richard Semon 在 1904 年提出，指的是大脑中存储记忆痕迹的物理结构——当你记住"巴黎是法国首都"这个事实时，这条信息就以某种物理形式（可能是特定的神经连接模式）存储在你的大脑里，这个物理痕迹就叫 engram。

DeepSeek 用这个名字，显然是想说：我们要给大模型装上真正的"记忆"。

说实话，看完之后我挺兴奋的——这篇论文的思路非常优雅，而且解决的是一个很根本的问题。更重要的是，它触及了一个认知科学的经典命题：记忆和思考是什么关系？

## 风格特征

- 口语化开场："DeepSeek又发论文了"
- 自问自答/抛问题：最后一句
- 个人态度："说实话，我挺兴奋的"
- 背景铺垫：先解释 Engram 再抛观点
